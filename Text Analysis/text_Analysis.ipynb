{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9579e6ac-3970-4433-a7d2-5a2ffb9a9432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests, uuid\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05915f59-c92d-45bb-a78c-579637133714",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Hesas Misr_coments.csv\")\n",
    "\n",
    "df = df[df[\"message\"].notnull()]\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce168644-d859-442e-9475-cd88a4566f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb29e82-8e0f-49ab-af3d-ce9d4e25bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_key = \"2c8e1ed333e24e7497ed4b92bfa57380\"\n",
    "translate_endpoint = \"https://api.cognitive.microsofttranslator.com/\"\n",
    "\n",
    "Text_key = \"b6bdcbefc8894568bed37af2f6f31029\"\n",
    "Text_endpoint = \"https://internship-final-analysos.cognitiveservices.azure.com/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5f123e-5de7-475d-beb2-f7757462bd33",
   "metadata": {},
   "source": [
    "#### authoricate text analysis client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325ad7a1-af24-4666-bac0-2dc59d6cfdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate_text_client():\n",
    "    \n",
    "    global Text_key\n",
    "    global Text_endpoint\n",
    "    \n",
    "    key = Text_key\n",
    "    endpoint = Text_endpoint\n",
    "\n",
    "    text_analytics_client = TextAnalyticsClient( endpoint=endpoint,  credential= AzureKeyCredential(key))\n",
    "    return text_analytics_client\n",
    "\n",
    "text_analysis_client = authenticate_text_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a063f05-1092-4f08-8228-5286468e280a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "009afe78-3597-43bb-a09c-97d4c0e655ee",
   "metadata": {},
   "source": [
    "## Translate function\n",
    "**This functiontakes sentment and translate it to Englist.**\n",
    "\n",
    "if any error occured due to access key, we can update it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2a9547-7e82-4dca-b6e9-9aa7acea41be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def translate(sent):\n",
    "    \n",
    "    global translate_key\n",
    "    global translate_endpoint\n",
    "    \n",
    "    try:\n",
    "     \n",
    "        # subscription key and endpoint\n",
    "        subscription_key = translate_key\n",
    "        endpoint = translate_endpoint   \n",
    "        \n",
    "        # select endpoint resource url url\n",
    "        constructed_url = endpoint + '/translate'\n",
    "        \n",
    "        # set request parameters and headers\n",
    "        params = {\n",
    "            'api-version': '3.0',\n",
    "            'to': 'en',\n",
    "            'toScript': 'latn'}  \n",
    "        \n",
    "        headers = {\n",
    "            'Ocp-Apim-Subscription-Key': subscription_key,\n",
    "            'Ocp-Apim-Subscription-Region': \"eastus\",\n",
    "            'Content-type': 'application/json',\n",
    "            'X-ClientTraceId': str(uuid.uuid4())}\n",
    "        \n",
    "        # set body {text or document to translate}.\n",
    "        body = [{\n",
    "            'text': sent}]\n",
    "        \n",
    "        # send the request and get data\n",
    "        request = requests.post(constructed_url, params=params, headers=headers, json=body)\n",
    "        response = request.json()\n",
    "        \n",
    "        # print the result and return it\n",
    "        print(response[0]['translations'][0]['text'])\n",
    "        return response[0]['translations'][0]['text']\n",
    "    \n",
    "    except ConnectionError:\n",
    "        print(\"do you want to update the key? y/n\")\n",
    "        ans = input()\n",
    "        if ans == \"y\":\n",
    "            print(\"enter new translate key>>   \")\n",
    "            translate_key = input()\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59111622-f7a8-47b5-ab39-034607743d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6905cba3-b255-4630-94ba-33c3f92d3419",
   "metadata": {},
   "source": [
    "### NER function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c2917-9cd7-46cd-bf6d-3aa751f046f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def NER(text):\n",
    "    global text_analysis_client\n",
    "    \n",
    "    ## add lemit to the comment or its spam\n",
    "    if len(text) > 750:\n",
    "        return True\n",
    "    \n",
    "    try:\n",
    "        # listing the text \n",
    "        documents = [text]\n",
    "        # send request with the text\n",
    "        NER_res = text_analysis_client.recognize_entities(documents = documents)[0]\n",
    "\n",
    "        try:\n",
    "            # check if hte result is person return True\n",
    "            # if any error its not human name\n",
    "            entity = NER_res.entities[0]\n",
    "            if entity.category == 'Person':\n",
    "                return True\n",
    "            else:\n",
    "              return False\n",
    "        except IndexError:\n",
    "            return False\n",
    "        \n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf35b3e-9f57-48ad-b42c-655200e0badf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ca4612f-d5a0-446f-816a-8cc5beea98c6",
   "metadata": {},
   "source": [
    "#### create data dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573d2a56-d86b-4410-9852-e0cb72fb4694",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        \"text\":[],\n",
    "        \"Translate\":[],\n",
    "        \"is_human\":[],\n",
    "        \"Sentiment\":[]\n",
    "   \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b1b0bf-3e91-49e4-a7f8-888c71381818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(file, text, data):\n",
    "    global text_analysis_client\n",
    "    \n",
    "    ## check if the text is human name\n",
    "    is_human = NER(text)\n",
    "    \n",
    "    ## add data to dict and wright it to file\n",
    "    data[\"text\"].append(text)\n",
    "    data[\"is_human\"].append(is_human)\n",
    "    print(\"\\nis human: {}\\n\".format(is_human))\n",
    "    file.write(\"\\nis human: {}\\n\".format(is_human))\n",
    "    \n",
    "    \n",
    "    # if the text is not human \n",
    "    # translate the text \n",
    "    # analyse text score\n",
    "    \n",
    "    if not(is_human):\n",
    "        \n",
    "        # translate the text/append the thanslation to dict\n",
    "        translated_sent = translate(text)\n",
    "        data[\"Translate\"].append(translated_sent)\n",
    "\n",
    "        \n",
    "        ## analyse translated text score\n",
    "        response =  text_analysis_client.analyze_sentiment(documents=[translated_sent])[0]\n",
    "        ## P1 is the overall sentment \n",
    "        p1 = \"Document Sentiment: {}\".format(response.sentiment)\n",
    "        ## P2 is every rate score \n",
    "        p2 = \"\\nOverall scores: positive={0:.2f}; neutral={1:.2f}; negative={2:.2f} \\n\".format(\n",
    "            response.confidence_scores.positive,\n",
    "            response.confidence_scores.neutral,\n",
    "            response.confidence_scores.negative)\n",
    "        \n",
    "        ## print teh score\n",
    "        print(p1)\n",
    "        print(p2)\n",
    "        \n",
    "        ##write/append score to the file/dict\n",
    "        file.write(\"Translate: {}\\n\".format(sent))\n",
    "        file.write(p1+\"\\n\")\n",
    "        file.write(p2)\n",
    "        data[\"Sentiment\"].append(\"{}\\n{}\".format(p1, p2))\n",
    "\n",
    "    \n",
    "    # if the result is human name\n",
    "    # will not ranslate and assume the text is neutral\n",
    "    else:\n",
    "        data[\"Translate\"].append(\"not\")\n",
    "        data[\"Sentiment\"].append(\"\"\"Document Sentiment: neutral\n",
    "                    Overall scores: positive=0.00; neutral=1.00; negative=0.00\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880ae9ff-3a90-4b9c-aa0c-24fa336be90f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a91ffd54-178f-4cf0-9401-65ba6c9cecde",
   "metadata": {},
   "source": [
    "### starting function\n",
    "\n",
    "the function recalll itself untill the start number is the end number\n",
    "\n",
    "> start from number to end number\n",
    ">\n",
    "> write tothe file and recall itself intil the start == end then return to the first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5900f5aa-acb7-4ad0-be39-f1893affd2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(file, start, end):\n",
    "    global data\n",
    "    if start == end:\n",
    "        return\n",
    "    try:\n",
    "        text = df.message[start]\n",
    "        file.write('{} \\n'.format(text))\n",
    "        \n",
    "        sentiment_analysis(file, text, data)\n",
    "        \n",
    "        file.write(\"\\n done {}\\n\".format(start))\n",
    "        start(file, start+1, end)\n",
    "        \n",
    "    except KeyError as e:\n",
    "        \n",
    "       print(\"finished, {}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe801a58-25bf-456d-87c1-28cf05c02706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "459b5296-76e8-4d1f-accb-449fbdbc4d63",
   "metadata": {},
   "source": [
    "### open file ands ave data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158a78de-f5c8-4cf8-98a6-859e5a1e6128",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = int(\"\")\n",
    "end   = int(\"\")\n",
    "\n",
    "with open('{}{}-{}.txt'.format(\"Hessas\",start,end-1),  'w', encoding='utf-8') as f:\n",
    "    \n",
    "    ## open start function\n",
    "    start(f, k-1000)\n",
    "    \n",
    "    # after exit fromstart function \n",
    "    # save the data to .csv file and .txt  file\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"{}{}-{}.csv\".format(\"Hessas\", start, end-1),index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c533b2-95c0-457a-88a1-370f240c6189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
